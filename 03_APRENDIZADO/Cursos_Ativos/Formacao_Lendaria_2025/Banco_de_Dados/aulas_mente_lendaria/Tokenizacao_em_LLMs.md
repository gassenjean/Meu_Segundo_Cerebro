---
criado: 2024-09-23T17:26:45-03:00
atualizado: 2025-07-09T20:27:08-03:00
---

#consumir

## Quer saber por que voc√™ deveria aprender isso?

- Por que o LLM n√£o consegue soletrar palavras? Tokeniza√ß√£o.
- Por que o LLM n√£o consegue realizar tarefas super simples de processamento de strings, como reverter uma string? Tokeniza√ß√£o.
- Por que o LLM √© pior em idiomas que n√£o s√£o em ingl√™s (por exemplo, japon√™s)? Tokeniza√ß√£o.
- Por que o LLM √© ruim em aritm√©tica simples? Tokeniza√ß√£o.
- Por que eu deveria preferir usar YAML ao inv√©s de JSON com LLMs? Tokeniza√ß√£o.
- Qual √© a verdadeira raiz do sofrimento? Tokeniza√ß√£o. üòÇ

Dominar a Tokeniza√ß√£o √© essencial para se comunicar de forma eficaz com IAs e extrair delas todo o potencial

Ent√£o bora aprender!

## Por que √© Importante?

Para entender por que a tokeniza√ß√£o √© importante, precisamos pensar em dois aspectos dos modelos implantados: limites de tokens e precifica√ß√£o de tokens.

Limites de Tokens. Cada modelo tem uma janela de contexto definida como o n√∫mero m√°ximo de tokens que pode processar por solicita√ß√£o √∫nica. Por exemplo, modelos antigos de gpt-3.5-turbo t√™m um limite de 4K tokens (contexto) para cada solicita√ß√£o. O limite de tokens √© compartilhado entre o prompt e a conclus√£o. Como a conclus√£o √© adicionada ao prompt para gerar o pr√≥ximo token, torna-se necess√°rio ajustar ambos dentro da janela de contexto total para uma √∫nica solicita√ß√£o.

Precifica√ß√£o de Tokens. Como em qualquer API, o uso da implanta√ß√£o do modelo incorre em custos baseados no tipo e vers√£o do modelo. Atualmente, a precifica√ß√£o do modelo est√° vinculada ao n√∫mero de tokens usados, com diferentes pontos de pre√ßo poss√≠veis para cada tipo ou vers√£o de modelo.

## Introdu√ß√£o √† Tokeniza√ß√£o:

No mundo dos LLMs, a tokeniza√ß√£o √© o processo de dividir textos em peda√ßos menores, conhecidos como tokens.

- Um token pode ser um √∫nico caractere, uma fra√ß√£o de palavra ou uma palavra inteira.
- Muitas palavras comuns s√£o representadas por um √∫nico token.
- Palavras menos comuns s√£o representadas por m√∫ltiplos tokens.

Imagine que cada palavra, ou at√© mesmo partes de palavras, √© um quebra-cabe√ßa. A tokeniza√ß√£o √© o que nos permite separar cada pe√ßa desse quebra-cabe√ßa para que a m√°quina possa entender e, posteriormente, juntar essas pe√ßas novamente para formar uma imagem completa.

Por exemplo, o texto acima tem 99 tokens, pode [testar aqui](https://tiktokenizer.vercel.app/?model=gpt-4-1106-preview).

![[Pasted image 20240923172921.png]]

A Import√¢ncia da Tokeniza√ß√£o:

Por que isso √© crucial?

Bem, sem a tokeniza√ß√£o, os LLMs seriam como um turista perdido em uma cidade estrangeira sem um mapa.

Os tokens s√£o como coordenadas no mapa, guiando o modelo atrav√©s da complexidade da linguagem humana, permitindo que ele fa√ßa previs√µes precisas sobre o que vem a seguir no texto ou como responder a uma pergunta.

Se liga agora no "mapa da IA" no texto da imagem que te mostrei no par√°grafo anterior: 2822, 29452, 8924, 445, 11237, 82, 11, 264, 4037, 48476, 4046, 297, 59996, 409, 29932, 404, 1495, 437, 991, 10696, 64, 49019, 3026, 4692, 11, 71583, 13652, 8112, 11460, 13, 38891, 1744, 19394, 95747, 11, 6033, 39583, 41349, 81282, 409, 11091, 89223, 11, 4046, 4543, 1744, 14720, 1824, 8393, 17930, 13, 362, 4037, 48476, 4046, 297, 1744, 12155, 52603, 4941, 277, 19394, 1069, 17930, 951, 325, 1744, 14720, 1824, 8393, 17930, 3429, 1744, 264, 29830, 54506, 2278, 64, 96537, 384, 11, 46000, 12826, 11, 503, 3935, 277, 4043, 300, 1069, 46378, 88019, 3429, 1376, 277, 10832, 63997, 71301, 13

Isso ai s√£o os Tokens.

Por exemplo, "No" √© 2882, " mundo" √© 29452, e assim por diante.

![[Pasted image 20240923173026.png]]

T√° dif√≠cil de entender? Vamos continuar que vai ficar mais f√°cil.

Imagine que voc√™ est√° jogando um videogame onde cada objeto, personagem ou a√ß√£o √© representado por um n√∫mero √∫nico. Em nosso jogo, o mundo da intelig√™ncia artificial, esses n√∫meros s√£o chamados de tokens, e o mapa que estamos seguindo √© composto exatamente por eles.

A sequ√™ncia num√©rica que mencionei antes, essa longa lista de n√∫meros, √© como se fosse um c√≥digo secreto. Cada n√∫mero representa uma pe√ßa espec√≠fica do nosso grande quebra-cabe√ßa de linguagem. Por exemplo, o n√∫mero 2822 pode representar a palavra "No", e 29452 pode representar " mundo". Esses n√∫meros n√£o s√£o aleat√≥rios; eles s√£o cuidadosamente selecionados e organizados por um processo chamado tokeniza√ß√£o.

Ent√£o, quando vemos ==2822, 29452, 8924, 445...==, n√£o estamos apenas olhando para n√∫meros; estamos vendo uma hist√≥ria sendo contada em uma linguagem que os modelos de IA, como os LLMs, podem entender. √â como se estiv√©ssemos traduzindo portugu√™s para o idioma da intelig√™ncia artificial.

Confesso que meu lado nerd gosta de entender sobre isso.

Mas se voc√™ n√£o entender isso, voc√™ vai perder muitas noites de sono, grana e talvez at√© cabelo. hehe

Ent√£o vamos continuar.

## A Ess√™ncia da Tokeniza√ß√£o:

Ap√≥s treinados, os tokenizadores t√™m duas fun√ß√µes primordiais: ==encode()==, que traduz de strings para tokens, e ==decode()==, que faz o caminho inverso.

Para tornar isso mais palp√°vel, imagine que voc√™ est√° enviando uma mensagem codificada a um amigo, onde cada palavra √© substitu√≠da por um n√∫mero baseado em um livro de c√≥digos que ambos t√™m. Somente quem tem o livro de c√≥digos (ou, neste caso, o algoritmo de tokeniza√ß√£o) pode traduzir essa mensagem de volta para o portugu√™s.

Isso √© exatamente o que acontece na tokeniza√ß√£o em LLMs. Transformamos texto em uma sequ√™ncia de n√∫meros (tokens) que o modelo pode "ler" e processar. E assim como em nosso jogo imagin√°rio, esses tokens s√£o a chave para desbloquear significados, responder perguntas, ou at√© mesmo criar textos novos e interessantes.

## M√©todos de Tokeniza√ß√£o:

- Tokeniza√ß√£o Baseada em Palavras: O m√©todo mais direto, dividindo o texto em palavras individuais. √â como desmontar um Lego, pe√ßa por pe√ßa, onde cada pe√ßa √© uma palavra.
- Tokeniza√ß√£o Baseada em Subpalavras: Essa t√©cnica vai um passo al√©m, dividindo palavras em fragmentos menores. Isso √© especialmente √∫til para lidar com palavras novas ou raras. Imagine cortar cada pe√ßa de Lego em partes ainda menores para entender melhor cada detalhe.
- Tokeniza√ß√£o Baseada em Caracteres: Aqui, cada caractere √© um token. √â como se diss√©ssemos que cada √°tomo de cada pe√ßa de Lego importa. Essa abordagem √© √∫til para idiomas com muitos caracteres, como o chin√™s.

## Desafios da Tokeniza√ß√£o:

- Efici√™ncia vs. Precis√£o: Encontrar o equil√≠brio certo entre cortar o texto em peda√ßos pequenos o suficiente para ser gerenci√°vel, mas n√£o t√£o pequenos que percam o significado, √© uma arte.
- Varia√ß√µes Lingu√≠sticas: Diferentes l√≠nguas e dialetos podem exigir abordagens de tokeniza√ß√£o distintas. √â como se as pe√ßas de Lego mudassem de forma dependendo da cidade que voc√™ est√° tentando construir.

## T√©cnicas Avan√ßadas de Prompt

Engenharia Reversa de Prompt Use ferramentas como o TikTokenizer para ver exatamente como seu prompt √© tokenizado. Compare diferentes formatos e veja qual √© mais eficiente.

Exemplo: JSON: {"produto": "camiseta", "descri√ß√£o": "100% algod√£o, v√°rias cores", "pre√ßo": 29.99} Fichas: 27

YAML: produto: camiseta Descri√ß√£o: 100% algod√£o, v√°rias cores Pre√ßo: 29.99 Fichas: 21

## Atividades Pr√°ticas:

Experimento de Tokeniza√ß√£o: Escolha um par√°grafo de texto e tente aplicar diferentes m√©todos de tokeniza√ß√£o manualmente. Observe como o significado √© afetado.

# Recursos Adicionais

## Ferramentas:

https://tiktokenizer.vercel.app/

Esse √© melhor v√≠deo sobre o assunto:

https://hundredblocks.github.io/transcription_demo/

https://www.youtube.com/watch?v=zduSFxRajkE&t=6711s

Fontes:

1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). Aten√ß√£o √© tudo que voc√™ precisa. Sistemas de processamento de informa√ß√µes neurais.
2. Sennrich, R., Haddow, B., & Birch, A. (2016). Tradu√ß√£o autom√°tica neural de palavras raras com unidades de subpalavras. Associa√ß√£o de Lingu√≠stica Computacional.

# ia

Este texto √© uma introdu√ß√£o ao conceito de tokeniza√ß√£o em modelos de linguagem, como os LLMs (Large Language Models). A tokeniza√ß√£o √© o processo de dividir textos em peda√ßos menores, conhecidos como tokens, que podem ser processados por modelos de linguagem.

**Por que a tokeniza√ß√£o √© importante?**

A tokeniza√ß√£o √© importante porque:

1. **Limites de tokens**: Cada modelo tem um limite de tokens que pode processar por solicita√ß√£o. A tokeniza√ß√£o ajuda a otimizar o uso desses tokens.
2. **Precifica√ß√£o de tokens**: A precifica√ß√£o de modelos est√° vinculada ao n√∫mero de tokens usados. A tokeniza√ß√£o ajuda a reduzir os custos.

**Introdu√ß√£o √† tokeniza√ß√£o**

A tokeniza√ß√£o √© o processo de dividir textos em peda√ßos menores, conhecidos como tokens. Um token pode ser um caractere, uma fra√ß√£o de palavra ou uma palavra inteira.

**M√©todos de tokeniza√ß√£o**

Existem diferentes m√©todos de tokeniza√ß√£o, incluindo:

1. **Tokeniza√ß√£o baseada em palavras**: divide o texto em palavras individuais.
2. **Tokeniza√ß√£o baseada em subpalavras**: divide palavras em fragmentos menores.
3. **Tokeniza√ß√£o baseada em caracteres**: divide o texto em caracteres individuais.

**Desafios da tokeniza√ß√£o**

A tokeniza√ß√£o apresenta desafios, incluindo:

1. **Efici√™ncia vs. precis√£o**: encontrar o equil√≠brio entre cortar o texto em peda√ßos pequenos o suficiente para ser gerenci√°vel, mas n√£o t√£o pequenos que percam o significado.
2. **Varia√ß√µes lingu√≠sticas**: diferentes l√≠nguas e dialetos podem exigir abordagens de tokeniza√ß√£o distintas.

**T√©cnicas avan√ßadas de prompt**

A engenharia reversa de prompt √© uma t√©cnica que envolve analisar como um prompt √© tokenizado e ajustar o prompt para otimizar o uso de tokens.

**Recursos adicionais**

O texto fornece recursos adicionais, incluindo:

1. **Ferramentas**: como o TikTokenizer, que pode ser usado para analisar como um prompt √© tokenizado.
2. **V√≠deos**: um v√≠deo sobre o assunto de tokeniza√ß√£o.
3. \*\*
